# Optimization Algorithms
For every Machine Learning model, after the loss function has been arrived at, we need to apply optimization to find the parameters, state that reduces the loss function. This method is accomplished by using Optimization Algorithms

# SGD
Stochastic Gradient Descent is the most commonly used algorithm

# SAG
Stochastic Average Gradient is a new phenomenon </br>
https://arxiv.org/abs/1309.2388 </br>
Paper PDF https://arxiv.org/pdf/1309.2388

# ADAM
Adam Optimizer is the latest craze in Optimizer algorithms </br>
https://arxiv.org/pdf/1412.6980.pdf
</br>
You can also take a look at my blog for better understanding
https://anantguptadbl.github.io/optimizers/2018/12/28/UnderstandingAdamOptimizer.html


