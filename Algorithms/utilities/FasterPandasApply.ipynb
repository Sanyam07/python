{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison DASK, SWIFTER and NORMAL APPLY\n",
    "\n",
    "# Sample data\n",
    "data=pd.DataFrame(np.random.rand(1000000,100),columns=['col'+str(x) for x in range(100)])\n",
    "data['id']=np.random.randint(1000,1000000)\n",
    "\n",
    "# NORMAL  APPLY\n",
    "%time data['resultnormal']=data.apply(lambda row : 0 if (row['col0']+row['col1']+row['col3']+row['col4']+row['col5']) < 0.5 else 1 if (row['col2']+row['col4']+row['col6']+row['col9']+row['col10']<0.8) else 2, axis=1)\n",
    "# 1min 28s\n",
    "\n",
    "# DASK  APPLY\n",
    "import dask.dataframe as dd\n",
    "ddf = dd.from_pandas(data, npartitions=128)\n",
    "def myFunc(row):\n",
    "    if(row['col0']+row['col1']+row['col3']+row['col4']+row['col5'] < 0.5):\n",
    "        return(0)\n",
    "    elif(row['col2']+row['col4']+row['col6']+row['col9']+row['col10']<0.8):\n",
    "        return(1)\n",
    "    else:\n",
    "        return(2)\n",
    "\n",
    "ddf['resultdask']=ddf.apply(myFunc, axis=1)\n",
    "%time ddf.compute()\n",
    "# 1min 33s 4 partitions\n",
    "# 1min 29s 8 partitions\n",
    "# 1min 23s 16 partitions\n",
    "# 1min 29s 128 partitions\n",
    "\n",
    "# SWIFTER\n",
    "import swifter\n",
    "%time data['resultnormal'] = data.swifter.apply(lambda row : 0 if (row['col0']+row['col1']+row['col3']+row['col4']+row['col5']) < 0.5 else 1 if (row['col2']+row['col4']+row['col6']+row['col9']+row['col10']<0.8) else 2, axis=1)\n",
    "# 41.5 s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NORMAL\n",
    "\n",
    "### Normal Pandas Apply happens sequentially\n",
    "\n",
    "https://github.com/pandas-dev/pandas/blob/master/pandas/core/apply.py\n",
    "\n",
    "    klass: Type[FrameApply]\n",
    "    if axis == 0:\n",
    "        klass = FrameRowApply\n",
    "    elif axis == 1:\n",
    "        klass = FrameColumnApply\n",
    "        \n",
    "### The function that is called is the apply_broadcast which uses a FOR loop\n",
    "def apply_broadcast(self, target: \"DataFrame\") -> \"DataFrame\":\n",
    "        result_values = np.empty_like(target.values)\n",
    "\n",
    "        # axis which we want to compare compliance\n",
    "        result_compare = target.shape[0]\n",
    "\n",
    "        for i, col in enumerate(target.columns):\n",
    "            res = self.f(target[col])\n",
    "            ares = np.asarray(res).ndim\n",
    "\n",
    "            # must be a scalar or 1d\n",
    "            if ares > 1:\n",
    "                raise ValueError(\"too many dims to broadcast\")\n",
    "            elif ares == 1:\n",
    "\n",
    "                # must match return dim\n",
    "                if result_compare != len(res):\n",
    "                    raise ValueError(\"cannot broadcast result\")\n",
    "\n",
    "            result_values[:, i] = res\n",
    "            \n",
    "# DASK\n",
    "\n",
    "### DASK is a behemoth which consists of a lot of configurations and parameters\n",
    "\n",
    "#### n_partitions\n",
    "This allows the number of separate groups that the indices of the pandas dataframe will be split into. It allows for separating out the chunks of the dataframe\n",
    "\n",
    "#### ddf.apply and compute\n",
    "Dask apply is a lazy eval. This means that executing the apply does not lead to a direct execution. Instead it just creates the execution graph. Based on the scheduler chosen, the parallelization happens\n",
    "The default scheduler for DASK is THREADS\n",
    "\n",
    "# SWIFTER\n",
    "\n",
    "### SWIFTER is nothing but a small scale dask\n",
    "It has two main defaults that makes it seem faster\n",
    " * Scheduler : Processes instead of Threads\n",
    " * Partitions : cpu_count() * 2\n",
    "\n",
    "### You can arrive at the same performance level with DASK with the following changes\n",
    "```\n",
    "ddf.map_partitions(myFunc,axis=1,meta=('resultdask',np.float32))\n",
    "with dask.config.set({\"multiprocessing.context\": \"fork\"}):\n",
    "    %time ddf.compute(scheduler=\"processes\")\n",
    "    \n",
    "#41 seconds\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
