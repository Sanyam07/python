{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ReviewExp",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anantguptadbl/python/blob/master/pytorch/explanability/ReviewExp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0xUF1_isuUN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.corpus import wordnet\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DhbXsxdx3-0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "96caf2a4-c704-4792-c8fd-a075bde95135"
      },
      "source": [
        "use_cuda=True\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnIwT6UAAgi8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "478f12f3-25a0-46ff-f05e-e9c26f80851e"
      },
      "source": [
        "with open(\"/content/gdrive/My Drive/Colab Notebooks/b.csv\",\"rb\") as file:\n",
        "  allData=file.readlines()\n",
        "\n",
        "allData=[str(x).replace('\\r','').replace('\\n','').replace('\\\\n','').replace('\\\\r','').replace('\\\\','') for x in allData]\n",
        "allData=allData[1:]\n",
        "allData=[x[x.find(',')+1:] for x in allData]\n",
        "allData=[x.replace('\\'','') for x in allData]\n",
        "ratings=[int(x[-1:]) for x in allData]\n",
        "allData=[x[:-1] for x in allData]\n",
        "allData=[x.replace('\"','').replace(\",\",'') for x in allData]\n",
        "\n",
        "# We will only take Adjective and Adverbs for our prediction\n",
        "allData=[[y[0] for y in nltk.pos_tag([x for x in z.split(' ') if len(x) > 2]) if y[1] in ['JJ','JJR','JJS','RB','RBR','RBS','NN','VB']] for z in allData]\n",
        "\n",
        "# Filtering the unique Words\n",
        "uniqueWords=np.unique(np.array([y for x in allData for y in x]),return_counts=True)\n",
        "uniqueWords=pd.DataFrame(zip(uniqueWords[0],uniqueWords[1]),columns=['word','wordCount'])\n",
        "uniqueWords=uniqueWords[uniqueWords['wordCount']>50]\n",
        "uniqueWords=uniqueWords['word'].values\n",
        "\n",
        "#uniqueWords=np.unique(np.array([y for x in allData for y in x]))\n",
        "uniqueWords=[x for x in uniqueWords if len(wordnet.synsets(x))>0]\n",
        "uniqueWords=[x for x in uniqueWords if x[0].isdigit()==False]\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "p = nltk.PorterStemmer()\n",
        "uniqueWordDict=dict((x,lemmatizer.lemmatize(x, pos=\"a\"))  for x in uniqueWords)\n",
        "\n",
        "def returnValidWords(curSentence,uniqueWordDict):\n",
        "  alteredSentence=[]\n",
        "  for curWord in curSentence:\n",
        "    if(curWord in uniqueWordDict):\n",
        "      alteredSentence.append(uniqueWordDict[curWord])\n",
        "  return(alteredSentence)\n",
        "\n",
        "allData=[returnValidWords(x,uniqueWordDict) for x in allData]\n",
        "uniqueWords=np.unique(list(uniqueWordDict.values()))\n",
        "uniqueWords=dict((x,i) for i,x in enumerate(uniqueWords))\n",
        "\n",
        "def getVector(curSentence,uniqueWords):\n",
        "  curArray=np.zeros(len(uniqueWords))\n",
        "  for curWord in curSentence:\n",
        "    curArray[uniqueWords[curWord]]=1\n",
        "  return(curArray)\n",
        "\n",
        "allData=[getVector(x,uniqueWords) for x in allData]\n",
        "allData=np.array(allData)\n",
        "ratings=np.array(ratings)\n",
        "\n",
        "print(\"Cell Execution Completed\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cell Execution Completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ws9Y40USP2BG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We will now be trying to fit a model and in the process derive explanability as well\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "class straightModel(nn.Module):\n",
        "  def __init__(self,inputDim):\n",
        "    super(straightModel,self).__init__()\n",
        "    self.inputDim=inputDim\n",
        "    self.l1=nn.Linear(self.inputDim,1024).cuda()\n",
        "    self.b1=nn.BatchNorm1d(1024).cuda()\n",
        "    self.l2=nn.Linear(1024,256).cuda()\n",
        "    self.b2=nn.BatchNorm1d(256).cuda()\n",
        "    self.l3=nn.Linear(256,64).cuda()\n",
        "    self.b3=nn.BatchNorm1d(64).cuda()\n",
        "    self.l4=nn.Linear(64,1).cuda()\n",
        "\n",
        "  def forward(self,x):\n",
        "    out=F.relu(self.b1(self.l1(x))).cuda()\n",
        "    out=F.relu(self.b2(self.l2(out))).cuda()\n",
        "    out=F.relu(self.b3(self.l3(out))).cuda()\n",
        "    out=self.l4(out).cuda()\n",
        "    return(out)\n",
        "\n",
        "# Model Configuration\n",
        "learning_rate=0.0001\n",
        "batchSize=256\n",
        "numBatches=int(len(allData)/batchSize)\n",
        "numEpochs=10000\n",
        "\n",
        "# Model\n",
        "model=straightModel(allData.shape[1]).cuda()\n",
        "criterion=nn.MSELoss()\n",
        "optimizer=torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
        "\n",
        "for curEpoch in range(numEpochs):\n",
        "  totalLoss=0\n",
        "  for curBatch in range(numBatches):\n",
        "    optimizer.zero_grad()\n",
        "    dataOutput = model(Variable(torch.from_numpy(allData[curBatch*batchSize:(curBatch+1)*batchSize].astype(np.float32))).cuda())\n",
        "    loss = criterion(dataOutput, Variable(torch.from_numpy(ratings[curBatch*batchSize:(curBatch+1)*batchSize].astype(np.float32))).cuda())\n",
        "    # We will perform the backward propagation\n",
        "    loss.backward()\n",
        "    totalLoss=totalLoss + loss.item()\n",
        "    optimizer.step()\n",
        "  if(curEpoch%50==0):\n",
        "      print('epoch {0} totalLoss {1}'.format(curEpoch + 1,totalLoss))   \n",
        "\n",
        "layerNames=['l1','l2','l3','l4']\n",
        "layersDict={}\n",
        "for layerName in layerNames:\n",
        "    for featureNumber,feature in enumerate(model._modules[layerName].weight.detach().numpy()):\n",
        "        curResult=Variable(torch.from_numpy(allData[0:1].astype(np.float32)))\n",
        "        for curLayerName in ['l1','l2','l3','l4']:\n",
        "            curLayer=model._modules[curLayerName]\n",
        "            if(curLayerName==layerName):\n",
        "                break\n",
        "            else:\n",
        "                print(\"Before {0} the result is {1}\".format(curLayer,curResult.size()))\n",
        "                print(model._modules[curLayerName])\n",
        "                curResult=curLayer(curResult)\n",
        "                print(\"After {0} the result is {1}\".format(curLayer,curResult.size()))\n",
        "        curResult=feature*curResult.detach().numpy()\n",
        "        curResult=np.absolute(curResult)\n",
        "        curResult=curResult/np.sum(curResult)\n",
        "        if(layerName not in layersDict):\n",
        "            layersDict[layerName]=[]\n",
        "        layersDict[layerName].append(curResult)\n",
        "        \n",
        "for curLayerIndex in range(len(layerNames)-1):\n",
        "    for curSubLayer in layersDict[layerNames[len(layerNames)-curLayerIndex-1]]:   \n",
        "        for i,x in enumerate(curSubLayer):\n",
        "            layersDict[layerNames[len(layerNames)-curLayerIndex-2]][i]=layersDict[layerNames[len(layerNames)-curLayerIndex-2]][i]*x[0]\n",
        "featureImportance=np.sum(np.array(layersDict[layerNames[0]]),axis=0)\n",
        "featureImportance=pd.DataFrame(zip(uniqueWords.keys(),featureImportance[0]),columns=['word','importance'])\n",
        "featureImportance.sort_values(by='importance',ascending=False)  "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}